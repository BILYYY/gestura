
# Gestura ğŸ¤Ÿ

**Real-Time Norwegian Sign Language Recognition System**

Gestura is an open-source computer vision system that recognizes Norwegian Sign Language hand gestures and converts them into typed text in real-time. Built with OpenCV and Python, it supports all 29 Norwegian letters (A-Ã…) plus SPACE and DELETE gestures.

![Gestura Demo](demo.gif)
*Real-time gesture recognition with skeleton overlay*

---

## âœ¨ Features

- **Norwegian Sign Language Support** - All 29 letters including Ã†, Ã˜, Ã…
- **Real-Time Recognition** - 30+ FPS with sub-100ms latency
- **Dual Skeleton Tracking** - CV mode (classical algorithms) or MediaPipe mode (deep learning)
- **Offline Operation** - No internet connection required
- **Adaptive Calibration** - Automatic or manual HSV tuning for different skin tones and lighting
- **System-Level Typing** - Works with any application (text editors, browsers, etc.)
- **Visual Feedback** - Live skeleton overlay, confidence meters, and subtitle display

---

## ğŸ¯ Quick Start

### Prerequisites

- Python 3.11+
- Webcam (720p recommended)
- Windows 10/11, macOS 10.14+, or Linux (Ubuntu 20.04+)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/BILYYY/gestura.git
   cd gestura
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Optional - Install MediaPipe for better accuracy:**
   ```bash
   pip install mediapipe
   ```

### Run Gestura

```bash
python run_gestura.py
```

On first launch:
1. Choose skeleton mode (1=CV, 2=MediaPipe)
2. Press `1` for simple calibration or `S` to skip
3. Show letter "A" and press SPACE when ready
4. Press `A` to activate typing mode
5. Start signing! ğŸ¤Ÿ

---

## ğŸ® Controls

| Key | Action |
|-----|--------|
| **A** | Toggle typing mode (ACTIVE/INACTIVE) |
| **K** | Re-run calibration |
| **H** | Toggle help overlay |
| **G** | Toggle guide box |
| **ESC** | Quit |

---

## ğŸ“¸ How It Works

```
Camera â†’ Hand Detection â†’ Skeleton Tracking â†’ Feature Extraction â†’ Recognition â†’ Typing
         (HSV + Morphology)  (CV/MediaPipe)     (ORB + Geometric)    (Temporal Filter)
```

### Recognition Pipeline

1. **Hand Detection** - HSV color space filtering + morphological operations
2. **Skeleton Tracking** - Fingertip detection (convex hull or MediaPipe)
3. **Feature Extraction** - Geometric features + ORB texture + skeleton data
4. **Hybrid Fusion** - Combines 40% geometric + 40% ORB + 20% skeleton
5. **Temporal Stabilization** - 3-tier filtering eliminates flickering
6. **Keyboard Output** - System-level typing via pynput

---

## ğŸ“Š Performance

| Metric | CV Mode | MediaPipe Mode |
|--------|---------|----------------|
| **Overall Accuracy** | 85-90% | 94-96% |
| **Close Fingers (M,N,W)** | 60-70% | 98-99% |
| **Frame Rate** | 34-36 FPS | 28-32 FPS |
| **Latency** | ~50ms | ~60ms |

*Tested on Intel i5 8th Gen CPU*

---

## ğŸ› ï¸ Project Structure

```
gestura/
â”œâ”€â”€ gestura/                   # Core modules
â”‚   â”œâ”€â”€ hand_detector.py       # Hand detection (HSV + morphology)
â”‚   â”œâ”€â”€ recognizer_orb.py      # Gesture recognition (hybrid fusion)
â”‚   â”œâ”€â”€ keyboard_manager.py    # Keyboard output
â”‚   â”œâ”€â”€ subtitle_manager.py    # Visual feedback
â”‚   â””â”€â”€ calibration.py         # Calibration wizard
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ _skeleton.py           # Skeleton tracking (CV + MediaPipe)
â”‚   â”œâ”€â”€ _shared_utils.py       # Helper functions
â”‚   â””â”€â”€ capture.py             # Reference capture tool
â”œâ”€â”€ resources/
â”‚   â”œâ”€â”€ references/            # Universal reference images
â”‚   â””â”€â”€ references_personal/   # Personal reference images
â”œâ”€â”€ requirements.txt
â””â”€â”€ run_gestura.py            # Main application
```

---

## ğŸ¨ Customization

### Capture Personal References

For better accuracy with your specific hand:

```bash
python tools/capture.py
```

Follow the 4-phase process:
1. Calibrate
2. Test quality
3. Choose "Personal" mode (press `2`)
4. Capture all letters A-Ã… + SPACE + DELETE

Personal references save to `resources/references_personal/`

### Manual Calibration

If automatic calibration fails:
1. Press `K` during main interface
2. Press `M` for manual HSV tuning
3. Adjust 6 sliders while watching live preview
4. Press SPACE to save

---

## ğŸ› Troubleshooting

### Hand Not Detected
- âœ… Re-run calibration (press `K`)
- âœ… Try manual HSV tuning (press `M`)
- âœ… Improve lighting (avoid backlighting)
- âœ… Use plain, neutral background

### Wrong Letters Recognized
- âœ… Capture personal references (`python tools/capture.py`)
- âœ… Enable MediaPipe mode for letters M, N, W
- âœ… Hold gesture steady (wait for confidence >70%)
- âœ… Check gesture matches reference alphabet

### Slow Performance
- âœ… Use CV mode instead of MediaPipe
- âœ… Close other camera applications
- âœ… Lower camera resolution

---

## ğŸ“š Documentation

Full documentation including system architecture, implementation details, and evaluation results is available in the `docs/` folder.

- [Installation Guide](docs/installation.md)
- [User Manual](docs/user_manual.md)
- [API Documentation](docs/api.md)
- [Technical Report](docs/report.pdf)

---

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:

- Multi-hand support
- Dynamic gesture recognition (motion-based signs)
- Additional sign languages
- Mobile platform ports
- Performance optimizations
- Bug fixes

**How to contribute:**
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ‘¥ Authors

- **[Elias Bouchabti ]** - [GitHub](https://github.com/BILYYY)
- **[Marthe]**
- **[Larsh]**
- **[Rafeal]**

---

## ğŸ™ Acknowledgments

- Norwegian Sign Language alphabet reference from [source]
- OpenCV community for excellent documentation
- MediaPipe team for hand landmark detection
- Course instructor for guidance and feedback

---


## âš ï¸ Known Limitations

- Single-hand recognition only (no two-handed signs)
- Static gestures only (no motion-based signs)
- Requires calibration per environment
- Hand must be roughly upright (Â±30Â° rotation)
- Success rate ~75% (depends on lighting and camera quality)

---

## ğŸ—ºï¸ Roadmap

- [ ] Mobile app (Android/iOS)
- [ ] Multi-hand support
- [ ] Dynamic gesture recognition
- [ ] Word prediction/autocomplete
- [ ] Additional sign languages (ASL, BSL, etc.)
- [ ] Deep learning end-to-end model
- [ ] Web-based version (WebAssembly)

---

## â­ Star History

If you find this project useful, please consider giving it a star! â­

[![Star History Chart](https://api.star-history.com/svg?repos=BILYYY/gestura&type=Date)](https://star-history.com/#BILYYY/gestura&Date)

---

<div align="center">

**Made with â¤ï¸ for the Norwegian deaf and hard-of-hearing community**

[Report Bug](https://github.com/BILYYY/gestura/issues) Â· [Request Feature](https://github.com/BILYYY/gestura/issues) Â· [Documentation](docs/)
